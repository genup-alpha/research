{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>MaskId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230930_21h03m03s_B01_1.png</td>\n",
       "      <td>20230930_21h03m03s_B01_1_mask.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230930_21h03m03s_B01_2.png</td>\n",
       "      <td>20230930_21h03m03s_B01_2_mask.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230930_21h03m06s_B02_1.png</td>\n",
       "      <td>20230930_21h03m06s_B02_1_mask.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230930_21h03m06s_B02_2.png</td>\n",
       "      <td>20230930_21h03m06s_B02_2_mask.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230930_21h03m06s_B02_3.png</td>\n",
       "      <td>20230930_21h03m06s_B02_3_mask.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ImageId                             MaskId\n",
       "0  20230930_21h03m03s_B01_1.png  20230930_21h03m03s_B01_1_mask.png\n",
       "1  20230930_21h03m03s_B01_2.png  20230930_21h03m03s_B01_2_mask.png\n",
       "2  20230930_21h03m06s_B02_1.png  20230930_21h03m06s_B02_1_mask.png\n",
       "3  20230930_21h03m06s_B02_2.png  20230930_21h03m06s_B02_2_mask.png\n",
       "4  20230930_21h03m06s_B02_3.png  20230930_21h03m06s_B02_3_mask.png"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"../DataSet/BeadSegmentation/original_data/\"\n",
    "data_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bead_id(x):\n",
    "    return x.split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>MaskId</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230930_21h03m03s_B01_1.png</td>\n",
       "      <td>20230930_21h03m03s_B01_1_mask.png</td>\n",
       "      <td>20230930_21h03m03s_B01_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230930_21h03m03s_B01_2.png</td>\n",
       "      <td>20230930_21h03m03s_B01_2_mask.png</td>\n",
       "      <td>20230930_21h03m03s_B01_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230930_21h03m06s_B02_1.png</td>\n",
       "      <td>20230930_21h03m06s_B02_1_mask.png</td>\n",
       "      <td>20230930_21h03m06s_B02_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230930_21h03m06s_B02_2.png</td>\n",
       "      <td>20230930_21h03m06s_B02_2_mask.png</td>\n",
       "      <td>20230930_21h03m06s_B02_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230930_21h03m06s_B02_3.png</td>\n",
       "      <td>20230930_21h03m06s_B02_3_mask.png</td>\n",
       "      <td>20230930_21h03m06s_B02_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>20231114_11h26m28s_B12_4.png</td>\n",
       "      <td>20231114_11h26m28s_B12_4_mask.png</td>\n",
       "      <td>20231114_11h26m28s_B12_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>20231114_11h26m31s_B13_1.png</td>\n",
       "      <td>20231114_11h26m31s_B13_1_mask.png</td>\n",
       "      <td>20231114_11h26m31s_B13_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>20231114_11h26m31s_B13_2.png</td>\n",
       "      <td>20231114_11h26m31s_B13_2_mask.png</td>\n",
       "      <td>20231114_11h26m31s_B13_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>20231114_11h26m31s_B13_3.png</td>\n",
       "      <td>20231114_11h26m31s_B13_3_mask.png</td>\n",
       "      <td>20231114_11h26m31s_B13_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>20231114_11h26m31s_B13_4.png</td>\n",
       "      <td>20231114_11h26m31s_B13_4_mask.png</td>\n",
       "      <td>20231114_11h26m31s_B13_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1394 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ImageId                             MaskId  \\\n",
       "0     20230930_21h03m03s_B01_1.png  20230930_21h03m03s_B01_1_mask.png   \n",
       "1     20230930_21h03m03s_B01_2.png  20230930_21h03m03s_B01_2_mask.png   \n",
       "2     20230930_21h03m06s_B02_1.png  20230930_21h03m06s_B02_1_mask.png   \n",
       "3     20230930_21h03m06s_B02_2.png  20230930_21h03m06s_B02_2_mask.png   \n",
       "4     20230930_21h03m06s_B02_3.png  20230930_21h03m06s_B02_3_mask.png   \n",
       "...                            ...                                ...   \n",
       "1389  20231114_11h26m28s_B12_4.png  20231114_11h26m28s_B12_4_mask.png   \n",
       "1390  20231114_11h26m31s_B13_1.png  20231114_11h26m31s_B13_1_mask.png   \n",
       "1391  20231114_11h26m31s_B13_2.png  20231114_11h26m31s_B13_2_mask.png   \n",
       "1392  20231114_11h26m31s_B13_3.png  20231114_11h26m31s_B13_3_mask.png   \n",
       "1393  20231114_11h26m31s_B13_4.png  20231114_11h26m31s_B13_4_mask.png   \n",
       "\n",
       "                            Id  \n",
       "0     20230930_21h03m03s_B01_1  \n",
       "1     20230930_21h03m03s_B01_2  \n",
       "2     20230930_21h03m06s_B02_1  \n",
       "3     20230930_21h03m06s_B02_2  \n",
       "4     20230930_21h03m06s_B02_3  \n",
       "...                        ...  \n",
       "1389  20231114_11h26m28s_B12_4  \n",
       "1390  20231114_11h26m31s_B13_1  \n",
       "1391  20231114_11h26m31s_B13_2  \n",
       "1392  20231114_11h26m31s_B13_3  \n",
       "1393  20231114_11h26m31s_B13_4  \n",
       "\n",
       "[1394 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"Id\"] = data_df.ImageId.apply(lambda x:extract_bead_id(x))\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1394"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bead_ids = np.unique(data_df.Id.values)\n",
    "len(bead_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bead_data(data_df, index):\n",
    "    client_ids = np.unique(data_df.Id.values)\n",
    "    client_id = client_ids[index]\n",
    "    client_data = data_df[data_df.Id == client_id]\n",
    "    image_files = list(client_data[\"ImageId\"])\n",
    "    mask_files = list(client_data[\"MaskId\"])\n",
    "    return client_id, image_files, mask_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('20230930_21h03m03s_B01_1',\n",
       " ['20230930_21h03m03s_B01_1.png'],\n",
       " ['20230930_21h03m03s_B01_1_mask.png'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "get_bead_data(data_df, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [\"background\", \"bead\"]\n",
    "colors = ((0,0,0), (255, 255, 255))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_bead_dataset(data_df, data_dir, regions, colors, index=0, image_size=448, wait_time=10000):\n",
    "    bead_id, image_files, mask_files = get_bead_data(data_df, index)\n",
    "\n",
    "    canvas = np.zeros(shape=(image_size, 2*image_size+50, 3), dtype=np.uint8)\n",
    "    window_name = 'Bead DataSet'\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    for i in range(len(image_files)):\n",
    "        image = cv2.imread(os.path.join(data_dir, \"images\", image_files[i]))\n",
    "        if image is None:\n",
    "            continue\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = cv2.imread(os.path.join(data_dir, \"masks\", mask_files[i]))\n",
    "        if mask is None:\n",
    "            continue\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        grid_pad = 50\n",
    "        canvas[:, :image_size, :] = image\n",
    "        canvas[:, image_size+grid_pad:2*image_size+grid_pad, :] = mask\n",
    "        \n",
    "        text_buff = 210\n",
    "        for j in range(1, len(regions)):\n",
    "            cv2.putText(canvas, f'{regions[j].upper()}', (900, text_buff), cv2.FONT_HERSHEY_SIMPLEX, 1, colors[j], 2)\n",
    "            text_buff += 40\n",
    "        \n",
    "        cv2.imshow(window_name, canvas)\n",
    "        key = cv2.waitKey(wait_time)\n",
    "        if key == 27:  # ESC key\n",
    "            break\n",
    "        if cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) < 1:  \n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# This function encapsulates the updated display mechanism. \n",
    "# It checks for the ESC key and window close event to exit. \n",
    "# Function calls are commented out to prevent execution at this stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_bead_dataset(data_df, data_dir, [\"region1\", \"region2\"], [(255,0,0), (0,255,0)], index=0, image_size=448, wait_time=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 255], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build tensor module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224    # U-net input image size: 224*224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class bead_dataset():\n",
    "    def __init__(self, data_dir, phase, transformer=None, split_ratio=0.1):\n",
    "        self.phase = phase\n",
    "        self.root_dir = data_dir\n",
    "        self.data_dir = os.path.join(data_dir, \"original_data\")  # 원본 데이터 폴더\n",
    "        self.images_dir = os.path.join(data_dir, phase, \"images\")\n",
    "        self.masks_dir = os.path.join(data_dir, phase, \"masks\")\n",
    "\n",
    "        # 원본 데이터 폴더 확인 및 train/val 데이터 폴더 생성\n",
    "        self.prepare_data_folders(split_ratio)\n",
    "        \n",
    "        # 이미지와 마스크 파일 목록 로드\n",
    "        self.image_files = [filename for filename in os.listdir(self.images_dir) if filename.endswith(\"png\")]\n",
    "        self.mask_files = [filename for filename in os.listdir(self.masks_dir) if filename.endswith(\"png\")]\n",
    "        assert len(self.image_files) == len(self.mask_files), \"The number of images and masks must be the same.\"\n",
    "\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def prepare_data_folders(self, split_ratio):\n",
    "        original_images = [f for f in os.listdir(os.path.join(self.data_dir, \"images\")) if f.endswith('png')]\n",
    "        original_masks = [f for f in os.listdir(os.path.join(self.data_dir, \"masks\")) if f.endswith('png')]\n",
    "\n",
    "        # 데이터 분할\n",
    "        train_images, val_images = train_test_split(original_images, test_size=split_ratio, random_state=42)\n",
    "\n",
    "        # 폴더 경로 설정\n",
    "        train_images_dir = os.path.join(self.root_dir, \"train\", \"images\")\n",
    "        train_masks_dir = os.path.join(self.root_dir, \"train\", \"masks\")\n",
    "        val_images_dir = os.path.join(self.root_dir, \"val\", \"images\")\n",
    "        val_masks_dir = os.path.join(self.root_dir, \"val\", \"masks\")\n",
    "\n",
    "        # 필요한 경우 폴더 생성\n",
    "        for directory in [train_images_dir, train_masks_dir, val_images_dir, val_masks_dir]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        # 이미지와 마스크 파일 복사\n",
    "        for img_list, img_dir, mask_dir in [(train_images, train_images_dir, train_masks_dir), (val_images, val_images_dir, val_masks_dir)]:\n",
    "            for img in img_list:\n",
    "                shutil.copy(os.path.join(self.data_dir, \"images\", img), img_dir)\n",
    "                \n",
    "                # 마스크 파일명 추정 및 복사\n",
    "                mask_name = img[:-4] + '_mask.png'\n",
    "                if mask_name in original_masks:\n",
    "                    shutil.copy(os.path.join(self.data_dir, \"masks\", mask_name), mask_dir)\n",
    "                else:\n",
    "                    print(f\"Warning: No matching mask file found for {img}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(os.path.join(self.images_dir, self.image_files[index]))\n",
    "        image = cv2.resize(image, dsize=(IMAGE_SIZE,IMAGE_SIZE), interpolation=cv2.INTER_LINEAR)\n",
    "        mask = cv2.imread(os.path.join(self.masks_dir, self.mask_files[index]))\n",
    "        mask = cv2.resize(mask, dsize=(IMAGE_SIZE,IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "        mask[mask < 240] = 0\n",
    "        mask[mask >= 240] = 255\n",
    "        mask = mask / 255\n",
    "\n",
    "        mask_H, mask_W, mask_C = mask.shape\n",
    "        background = np.ones((mask_H, mask_W))\n",
    "        background[mask[...,0] != 0] = 0\n",
    "        background[mask[...,1] != 0] = 0\n",
    "        background[mask[...,2] != 0] = 0\n",
    "        mask = np.concatenate([np.expand_dims(background, -1), mask], axis=-1)\n",
    "        mask = np.argmax(mask, axis=-1)\n",
    "\n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "        target = torch.from_numpy(mask).long()\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../DataSet/BeadSegmentation/\"\n",
    "dset = bead_dataset(data_dir, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[212, 210, 185],\n",
       "         [200, 208, 179],\n",
       "         [192, 195, 176],\n",
       "         ...,\n",
       "         [176, 201, 182],\n",
       "         [173, 194, 174],\n",
       "         [177, 199, 173]],\n",
       " \n",
       "        [[227, 233, 200],\n",
       "         [218, 235, 203],\n",
       "         [207, 214, 194],\n",
       "         ...,\n",
       "         [175, 195, 177],\n",
       "         [178, 199, 177],\n",
       "         [182, 204, 177]],\n",
       " \n",
       "        [[224, 238, 199],\n",
       "         [211, 233, 193],\n",
       "         [206, 214, 184],\n",
       "         ...,\n",
       "         [175, 199, 172],\n",
       "         [176, 203, 172],\n",
       "         [179, 201, 172]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[255, 255, 238],\n",
       "         [255, 255, 241],\n",
       "         [255, 253, 237],\n",
       "         ...,\n",
       "         [143, 164, 150],\n",
       "         [149, 173, 159],\n",
       "         [144, 165, 152]],\n",
       " \n",
       "        [[253, 254, 234],\n",
       "         [255, 255, 232],\n",
       "         [255, 253, 226],\n",
       "         ...,\n",
       "         [142, 157, 148],\n",
       "         [147, 164, 154],\n",
       "         [147, 162, 152]],\n",
       " \n",
       "        [[253, 254, 232],\n",
       "         [253, 254, 227],\n",
       "         [254, 252, 222],\n",
       "         ...,\n",
       "         [146, 169, 160],\n",
       "         [147, 170, 161],\n",
       "         [143, 160, 152]]], dtype=uint8),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def build_transformer():\n",
    "    transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = build_transformer()\n",
    "dset = bead_dataset(data_dir=data_dir, phase=\"train\", transformer=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.5125, 1.3070, 1.1700,  ..., 0.8961, 0.8447, 0.9132],\n",
       "          [1.7694, 1.6153, 1.4269,  ..., 0.8789, 0.9303, 0.9988],\n",
       "          [1.7180, 1.4954, 1.4098,  ..., 0.8789, 0.8961, 0.9474],\n",
       "          ...,\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 0.3309, 0.4337, 0.3481],\n",
       "          [2.2147, 2.2489, 2.2489,  ..., 0.3138, 0.3994, 0.3994],\n",
       "          [2.2147, 2.2147, 2.2318,  ..., 0.3823, 0.3994, 0.3309]],\n",
       " \n",
       "         [[1.6408, 1.6057, 1.3782,  ..., 1.4832, 1.3606, 1.4482],\n",
       "          [2.0434, 2.0784, 1.7108,  ..., 1.3782, 1.4482, 1.5357],\n",
       "          [2.1310, 2.0434, 1.7108,  ..., 1.4482, 1.5182, 1.4832],\n",
       "          ...,\n",
       "          [2.4286, 2.4286, 2.3936,  ..., 0.8354, 0.9930, 0.8529],\n",
       "          [2.4111, 2.4286, 2.3936,  ..., 0.7129, 0.8354, 0.8004],\n",
       "          [2.4111, 2.4111, 2.3761,  ..., 0.9230, 0.9405, 0.7654]],\n",
       " \n",
       "         [[1.4200, 1.3154, 1.2631,  ..., 1.3677, 1.2282, 1.2108],\n",
       "          [1.6814, 1.7337, 1.5768,  ..., 1.2805, 1.2805, 1.2805],\n",
       "          [1.6640, 1.5594, 1.4025,  ..., 1.1934, 1.1934, 1.1934],\n",
       "          ...,\n",
       "          [2.3437, 2.3960, 2.3263,  ..., 0.8099, 0.9668, 0.8448],\n",
       "          [2.2740, 2.2391, 2.1346,  ..., 0.7751, 0.8797, 0.8448],\n",
       "          [2.2391, 2.1520, 2.0648,  ..., 0.9842, 1.0017, 0.8448]]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for a, b in batch:\n",
    "        images.append(a)\n",
    "        targets.append(b)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    targets = torch.stack(targets, dim=0)\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([3, 224, 224])\n",
      "target shape: torch.Size([224, 224])\n"
     ]
    }
   ],
   "source": [
    "image, target = dset[0]\n",
    "print(f\"image shape: {image.shape}\")\n",
    "print(f\"target shape: {target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = DataLoader(dset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([4, 3, 224, 224])\n",
      "targets shape: torch.Size([4, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for index, batch in enumerate(dloader):\n",
    "    images = batch[0]\n",
    "    targets = batch[1]\n",
    "    print(f\"images shape: {images.shape}\")\n",
    "    print(f\"targets shape: {targets.shape}\")\n",
    "    \n",
    "    if index == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(data_dir, batch_size=4):\n",
    "    transformer = build_transformer()\n",
    "    \n",
    "    dataloaders = {}\n",
    "    train_dataset = bead_dataset(data_dir=data_dir, phase=\"train\", transformer=transformer)\n",
    "    dataloaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)   # 16,000여개 데이터 중 앞 120개 데이터만 사용하기 위해 Shuffle을 하지 않음(Segmentation code와 차이점)\n",
    "    \n",
    "    val_dataset = bead_dataset(data_dir=data_dir, phase=\"val\", transformer=transformer)\n",
    "    dataloaders[\"val\"] = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([4, 3, 224, 224])\n",
      "targets shape: torch.Size([4, 224, 224])\n",
      "images shape: torch.Size([4, 3, 224, 224])\n",
      "targets shape: torch.Size([4, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "dataloaders = build_dataloader(data_dir=data_dir) # 기본 batch 크기 4\n",
    "# dataloaders = build_dataloader(data_dir=data_dir, batch_size=6) # batch 크기 6\n",
    "\n",
    "for phase in [\"train\", \"val\"]:\n",
    "    for index, batch in enumerate(dataloaders[phase]):\n",
    "        images = batch[0]\n",
    "        targets = batch[1]\n",
    "        print(f\"images shape: {images.shape}\")\n",
    "        print(f\"targets shape: {targets.shape}\")\n",
    "        \n",
    "        if index == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. U-Net based on VGG16 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvLayer(in_channels, out_channels, kernel_size=3, padding=1):\n",
    "    layers = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    return layers\n",
    "\n",
    "def UpConvLayer(in_channels, out_channels):\n",
    "    layers = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/club8080/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/club8080/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16 = models.vgg16_bn(pretrained=False)\n",
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super().__init__()\n",
    "        backbone = models.vgg16_bn(pretrained=pretrained).features\n",
    "        self.conv_block1 = nn.Sequential(*backbone[:6])\n",
    "        self.conv_block2 = nn.Sequential(*backbone[6:13])\n",
    "        self.conv_block3 = nn.Sequential(*backbone[13:20])\n",
    "        self.conv_block4 = nn.Sequential(*backbone[20:27])\n",
    "        self.conv_block5 = nn.Sequential(*backbone[27:34], \n",
    "                                         ConvLayer(512, 1024, kernel_size=1, padding=0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        encode_features = []\n",
    "        out = self.conv_block1(x)\n",
    "        encode_features.append(out)\n",
    "        \n",
    "        out = self.conv_block2(out)\n",
    "        encode_features.append(out)\n",
    "        \n",
    "        out = self.conv_block3(out)\n",
    "        encode_features.append(out)\n",
    "        \n",
    "        out = self.conv_block4(out)\n",
    "        encode_features.append(out)\n",
    "        \n",
    "        out = self.conv_block5(out)\n",
    "        return out, encode_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy encoder test\n",
    "\n",
    "encoder = Encoder(pretrained=False)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "out, ftrs = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 224, 224])\n",
      "torch.Size([1, 128, 112, 112])\n",
      "torch.Size([1, 256, 56, 56])\n",
      "torch.Size([1, 512, 28, 28])\n",
      "torch.Size([1, 1024, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# feature 확인\n",
    "\n",
    "for ftr in ftrs:\n",
    "    print(ftr.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.upconv_layer1 = UpConvLayer(in_channels=1024, out_channels=512)\n",
    "        self.conv_block1 = ConvLayer(in_channels=512+512, out_channels=512)\n",
    "        \n",
    "        self.upconv_layer2 = UpConvLayer(in_channels=512, out_channels=256)\n",
    "        self.conv_block2 = ConvLayer(in_channels=256+256, out_channels=256)\n",
    "        \n",
    "        self.upconv_layer3 = UpConvLayer(in_channels=256, out_channels=128)\n",
    "        self.conv_block3 = ConvLayer(in_channels=128+128, out_channels=128)\n",
    "        \n",
    "        self.upconv_layer4 = UpConvLayer(in_channels=128, out_channels=64)\n",
    "        self.conv_block4 = ConvLayer(in_channels=64+64, out_channels=64)\n",
    "        \n",
    "    def forward(self, x, encoder_features):     # encoder의 feature가 함께 입력값으로 들어감\n",
    "        out = self.upconv_layer1(x)\n",
    "        out = torch.cat([out, encoder_features[-1]], dim=1)\n",
    "        out = self.conv_block1(out)\n",
    "        \n",
    "        out = self.upconv_layer2(out)\n",
    "        out = torch.cat([out, encoder_features[-2]], dim=1)\n",
    "        out = self.conv_block2(out)\n",
    "        \n",
    "        out = self.upconv_layer3(out)\n",
    "        out = torch.cat([out, encoder_features[-3]], dim=1)\n",
    "        out = self.conv_block3(out)\n",
    "        \n",
    "        out = self.upconv_layer4(out)\n",
    "        out = torch.cat([out, encoder_features[-4]], dim=1)\n",
    "        out = self.conv_block4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy test\n",
    "\n",
    "encoder = Encoder(pretrained=False)\n",
    "decoder = Decoder()\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "out, ftrs = encoder(x)\n",
    "out = decoder(out, ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(pretrained)\n",
    "        self.decoder = Decoder()\n",
    "        self.head = nn.Conv2d(64, num_classes, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, encode_features = self.encoder(x)\n",
    "        out = self.decoder(out, encode_features)\n",
    "        out = self.head(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy test\n",
    "model = UNet(num_classes=4, pretrained=False)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic segmentation Loss and train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F         # Loss 함수 구현을 위해 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_metric():\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.CE_loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        \n",
    "    def __call__(self, pred, target):\n",
    "        loss1 = self.CE_loss(pred, target)      # cross entropy loss\n",
    "        onehot_pred = F.one_hot(torch.argmax(pred, dim=1), num_classes=self.num_classes).permute(0, 3, 1, 2)\n",
    "        onehot_target = F.one_hot(target, num_classes=self.num_classes).permute(0, 3, 1, 2)\n",
    "        loss2 = self._get_dice_loss(onehot_pred, onehot_target) # dice coefficient loss\n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        dice_coefficient = self._get_batch_dice_coefficient(onehot_pred, onehot_target)\n",
    "        return loss, dice_coefficient\n",
    "    \n",
    "    def _get_dice_coeffient(self, pred, target):\n",
    "        set_inter = torch.dot(pred.reshape(-1).float(), target.reshape(-1).float())\n",
    "        set_sum = pred.sum() + target.sum()\n",
    "        if set_sum.item() == 0:\n",
    "            set_sum = 2 * set_inter\n",
    "        dice_coeff = (2 * set_inter) / (set_sum + 1e-9)\n",
    "        return dice_coeff\n",
    "    \n",
    "    def _get_multiclass_dice_coefficient(self, pred, target):\n",
    "        dice = 0\n",
    "        for class_index in range(1, self.num_classes):              # 0은 background에 대한 것으로 학습할 필요가 없음\n",
    "            dice += self._get_dice_coeffient(pred[class_index], target[class_index])\n",
    "        return dice / (self.num_classes - 1)\n",
    "    \n",
    "    def _get_batch_dice_coefficient(self, pred, target):\n",
    "        num_batch = pred.shape[0]\n",
    "        dice = 0\n",
    "        for batch_index in range(num_batch):\n",
    "            dice += self._get_multiclass_dice_coefficient(pred[batch_index], target[batch_index])\n",
    "        return dice / num_batch\n",
    "    \n",
    "    def _get_dice_loss(self, pred, target):\n",
    "        return 1 - self._get_batch_dice_coefficient(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([4, 3, 224, 224])\n",
      "targets shape: torch.Size([4, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# dummy test\n",
    "\n",
    "dataloaders = build_dataloader(data_dir=data_dir) # 기본 batch 크기 4\n",
    "# dataloaders = build_dataloader(data_dir=data_dir, batch_size=6) # batch 크기 6\n",
    "\n",
    "for index, batch in enumerate(dataloaders[phase]):\n",
    "    images = batch[0]\n",
    "    targets = batch[1]\n",
    "    predictions = model(images)\n",
    "\n",
    "    print(f\"images shape: {images.shape}\")\n",
    "    print(f\"targets shape: {targets.shape}\")\n",
    "    \n",
    "    if index == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3579, grad_fn=<AddBackward0>), tensor(0.0986))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy test\n",
    "\n",
    "criterion = UNet_metric(num_classes=4)\n",
    "criterion(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloaders, model, criterion, optimizer, device):\n",
    "    losses = {}\n",
    "    dice_coefficients = {}\n",
    "    \n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        running_loss = 0.0\n",
    "        running_dice_coeff = 0.0\n",
    "        \n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        for index, batch in enumerate(dataloaders[phase]):\n",
    "            images = batch[0].to(device)\n",
    "            targets = batch[1].to(device)\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                predictions = model(images)\n",
    "                loss, dice_coefficient = criterion(predictions, targets)\n",
    "                \n",
    "            if phase == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_dice_coeff += dice_coefficient.item()\n",
    "\n",
    "            if index == 10: # 10 index * mini_batch 데이터수 만큼 데이터를 한정\n",
    "                break\n",
    "\n",
    "        losses[phase] = running_loss / index\n",
    "        dice_coefficients[phase] = running_dice_coeff / index\n",
    "        \n",
    "    return losses, dice_coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Weight Initialization 과 Transfer learning 모델 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = True\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 12\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() and is_cuda else 'cpu')\n",
    "\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1. He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def He_initialization(module):\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(module.weight) # He initialization\n",
    "    elif isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = build_dataloader(data_dir, batch_size=BATCH_SIZE)\n",
    "model = UNet(num_classes=NUM_CLASSES, pretrained=False)\n",
    "model.apply(He_initialization)\n",
    "model = model.to(DEVICE)\n",
    "criterion = UNet_metric(num_classes=NUM_CLASSES)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/30 - Train loss: 1.6327, Val loss: 1.5494, Train dice: 0.3511, Val dice: 0.3377\n",
      "1/30 - Train loss: 1.5336, Val loss: 1.4982, Train dice: 0.3226, Val dice: 0.2702\n",
      "2/30 - Train loss: 1.5052, Val loss: 1.5128, Train dice: 0.2623, Val dice: 0.2209\n",
      "3/30 - Train loss: 1.5188, Val loss: 1.5170, Train dice: 0.2098, Val dice: 0.2089\n",
      "4/30 - Train loss: 1.5245, Val loss: 1.5150, Train dice: 0.1874, Val dice: 0.2052\n",
      "5/30 - Train loss: 1.5252, Val loss: 1.5162, Train dice: 0.1781, Val dice: 0.1979\n",
      "6/30 - Train loss: 1.5248, Val loss: 1.5190, Train dice: 0.1729, Val dice: 0.1900\n",
      "7/30 - Train loss: 1.5244, Val loss: 1.5216, Train dice: 0.1690, Val dice: 0.1833\n",
      "8/30 - Train loss: 1.5235, Val loss: 1.5231, Train dice: 0.1662, Val dice: 0.1784\n",
      "9/30 - Train loss: 1.5223, Val loss: 1.5235, Train dice: 0.1641, Val dice: 0.1750\n",
      "10/30 - Train loss: 1.5210, Val loss: 1.5239, Train dice: 0.1624, Val dice: 0.1720\n",
      "11/30 - Train loss: 1.5200, Val loss: 1.5241, Train dice: 0.1608, Val dice: 0.1694\n",
      "12/30 - Train loss: 1.5190, Val loss: 1.5242, Train dice: 0.1595, Val dice: 0.1672\n",
      "13/30 - Train loss: 1.5184, Val loss: 1.5241, Train dice: 0.1580, Val dice: 0.1651\n",
      "14/30 - Train loss: 1.5178, Val loss: 1.5241, Train dice: 0.1566, Val dice: 0.1634\n",
      "15/30 - Train loss: 1.5172, Val loss: 1.5238, Train dice: 0.1554, Val dice: 0.1619\n",
      "16/30 - Train loss: 1.5166, Val loss: 1.5238, Train dice: 0.1543, Val dice: 0.1604\n",
      "17/30 - Train loss: 1.5158, Val loss: 1.5240, Train dice: 0.1533, Val dice: 0.1588\n",
      "18/30 - Train loss: 1.5151, Val loss: 1.5244, Train dice: 0.1523, Val dice: 0.1570\n",
      "19/30 - Train loss: 1.5148, Val loss: 1.5245, Train dice: 0.1512, Val dice: 0.1558\n",
      "20/30 - Train loss: 1.5145, Val loss: 1.5248, Train dice: 0.1502, Val dice: 0.1543\n",
      "21/30 - Train loss: 1.5141, Val loss: 1.5247, Train dice: 0.1494, Val dice: 0.1532\n",
      "22/30 - Train loss: 1.5134, Val loss: 1.5247, Train dice: 0.1489, Val dice: 0.1520\n",
      "23/30 - Train loss: 1.5130, Val loss: 1.5246, Train dice: 0.1481, Val dice: 0.1512\n",
      "24/30 - Train loss: 1.5127, Val loss: 1.5246, Train dice: 0.1473, Val dice: 0.1503\n",
      "25/30 - Train loss: 1.5123, Val loss: 1.5245, Train dice: 0.1467, Val dice: 0.1495\n",
      "26/30 - Train loss: 1.5115, Val loss: 1.5248, Train dice: 0.1464, Val dice: 0.1484\n",
      "27/30 - Train loss: 1.5110, Val loss: 1.5247, Train dice: 0.1458, Val dice: 0.1477\n",
      "28/30 - Train loss: 1.5106, Val loss: 1.5248, Train dice: 0.1452, Val dice: 0.1468\n",
      "29/30 - Train loss: 1.5099, Val loss: 1.5248, Train dice: 0.1449, Val dice: 0.1460\n"
     ]
    }
   ],
   "source": [
    "train_loss_def, train_dice_coefficient_def = [], []\n",
    "val_loss_def, val_dice_coefficient_def = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses, dice_coefficients = train_one_epoch(dataloaders, model, criterion, optimizer, DEVICE)\n",
    "    train_loss_def.append(losses[\"train\"])\n",
    "    val_loss_def.append(losses[\"val\"])\n",
    "    train_dice_coefficient_def.append(dice_coefficients[\"train\"])\n",
    "    val_dice_coefficient_def.append(dice_coefficients[\"val\"])\n",
    "    \n",
    "    print(f\"{epoch}/{num_epochs} - Train loss: {losses['train']:.4f}, Val loss: {losses['val']:.4f},\" + \\\n",
    "          f\" Train dice: {dice_coefficients['train']:.4f}, Val dice: {dice_coefficients['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. Weight transfer pre-trained on ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Initialization보다 성능이 조금 더 낫다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = build_dataloader(data_dir, batch_size=BATCH_SIZE)\n",
    "model = UNet(num_classes=NUM_CLASSES, pretrained=True)          # pretrained True!!!!\n",
    "model = model.to(DEVICE)\n",
    "criterion = UNet_metric(num_classes=NUM_CLASSES)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/30 - Train loss: 1.6120, Val loss: 1.7831, Train dice: 0.1386, Val dice: 0.0517\n",
      "1/30 - Train loss: 1.6468, Val loss: 1.7590, Train dice: 0.0701, Val dice: 0.0043\n",
      "2/30 - Train loss: 1.6624, Val loss: 1.7019, Train dice: 0.0309, Val dice: 0.0023\n",
      "3/30 - Train loss: 1.6651, Val loss: 1.6796, Train dice: 0.0159, Val dice: 0.0040\n",
      "4/30 - Train loss: 1.6630, Val loss: 1.6715, Train dice: 0.0105, Val dice: 0.0061\n",
      "5/30 - Train loss: 1.6585, Val loss: 1.6682, Train dice: 0.0089, Val dice: 0.0068\n",
      "6/30 - Train loss: 1.6520, Val loss: 1.6667, Train dice: 0.0094, Val dice: 0.0065\n",
      "7/30 - Train loss: 1.6436, Val loss: 1.6636, Train dice: 0.0113, Val dice: 0.0069\n",
      "8/30 - Train loss: 1.6329, Val loss: 1.6580, Train dice: 0.0148, Val dice: 0.0081\n",
      "9/30 - Train loss: 1.6195, Val loss: 1.6500, Train dice: 0.0200, Val dice: 0.0104\n",
      "10/30 - Train loss: 1.6024, Val loss: 1.6391, Train dice: 0.0277, Val dice: 0.0141\n",
      "11/30 - Train loss: 1.5806, Val loss: 1.6256, Train dice: 0.0391, Val dice: 0.0195\n",
      "12/30 - Train loss: 1.5542, Val loss: 1.6094, Train dice: 0.0540, Val dice: 0.0269\n",
      "13/30 - Train loss: 1.5231, Val loss: 1.5922, Train dice: 0.0727, Val dice: 0.0359\n",
      "14/30 - Train loss: 1.4883, Val loss: 1.5750, Train dice: 0.0948, Val dice: 0.0455\n",
      "15/30 - Train loss: 1.4492, Val loss: 1.5601, Train dice: 0.1211, Val dice: 0.0540\n",
      "16/30 - Train loss: 1.4054, Val loss: 1.5479, Train dice: 0.1520, Val dice: 0.0616\n",
      "17/30 - Train loss: 1.3551, Val loss: 1.5366, Train dice: 0.1892, Val dice: 0.0701\n",
      "18/30 - Train loss: 1.2936, Val loss: 1.5262, Train dice: 0.2370, Val dice: 0.0799\n",
      "19/30 - Train loss: 1.2139, Val loss: 1.5156, Train dice: 0.3018, Val dice: 0.0923\n",
      "20/30 - Train loss: 1.1137, Val loss: 1.5055, Train dice: 0.3856, Val dice: 0.1070\n",
      "21/30 - Train loss: 0.9949, Val loss: 1.4970, Train dice: 0.4859, Val dice: 0.1233\n",
      "22/30 - Train loss: 0.8651, Val loss: 1.4961, Train dice: 0.5946, Val dice: 0.1369\n",
      "23/30 - Train loss: 0.7331, Val loss: 1.5009, Train dice: 0.7025, Val dice: 0.1495\n",
      "24/30 - Train loss: 0.6088, Val loss: 1.5143, Train dice: 0.7993, Val dice: 0.1588\n",
      "25/30 - Train loss: 0.4974, Val loss: 1.5136, Train dice: 0.8803, Val dice: 0.1788\n",
      "26/30 - Train loss: 0.4023, Val loss: 1.4728, Train dice: 0.9429, Val dice: 0.2243\n",
      "27/30 - Train loss: 0.3221, Val loss: 1.4591, Train dice: 0.9898, Val dice: 0.2501\n",
      "28/30 - Train loss: 0.2596, Val loss: 1.4801, Train dice: 1.0215, Val dice: 0.2502\n",
      "29/30 - Train loss: 0.2128, Val loss: 1.4763, Train dice: 1.0415, Val dice: 0.2641\n"
     ]
    }
   ],
   "source": [
    "train_loss_prt, train_dice_coefficient_prt = [], []\n",
    "val_loss_prt, val_dice_coefficient_prt = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses, dice_coefficients = train_one_epoch(dataloaders, model, criterion, optimizer, DEVICE)\n",
    "    train_loss_prt.append(losses[\"train\"])\n",
    "    val_loss_prt.append(losses[\"val\"])\n",
    "    train_dice_coefficient_prt.append(dice_coefficients[\"train\"])\n",
    "    val_dice_coefficient_prt.append(dice_coefficients[\"val\"])\n",
    "    \n",
    "    print(f\"{epoch}/{num_epochs} - Train loss: {losses['train']:.4f}, Val loss: {losses['val']:.4f},\" + \\\n",
    "          f\" Train dice: {dice_coefficients['train']:.4f}, Val dice: {dice_coefficients['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. Weight transfer with freezing encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = build_dataloader(data_dir, batch_size=BATCH_SIZE)\n",
    "model = UNet(num_classes=NUM_CLASSES, pretrained=True)\n",
    "model = model.to(DEVICE)\n",
    "model.encoder.requires_grad_ = False\n",
    "criterion = UNet_metric(num_classes=NUM_CLASSES)\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/30 - Train loss: 1.5833, Val loss: 1.8320, Train dice: 0.3408, Val dice: 0.0018\n",
      "1/30 - Train loss: 1.5705, Val loss: 1.7811, Train dice: 0.2294, Val dice: 0.0020\n",
      "2/30 - Train loss: 1.6533, Val loss: 1.7261, Train dice: 0.0690, Val dice: 0.0018\n",
      "3/30 - Train loss: 1.6756, Val loss: 1.6952, Train dice: 0.0173, Val dice: 0.0011\n",
      "4/30 - Train loss: 1.6743, Val loss: 1.6814, Train dice: 0.0079, Val dice: 0.0015\n",
      "5/30 - Train loss: 1.6709, Val loss: 1.6761, Train dice: 0.0056, Val dice: 0.0029\n",
      "6/30 - Train loss: 1.6666, Val loss: 1.6741, Train dice: 0.0050, Val dice: 0.0037\n",
      "7/30 - Train loss: 1.6615, Val loss: 1.6720, Train dice: 0.0051, Val dice: 0.0038\n",
      "8/30 - Train loss: 1.6557, Val loss: 1.6686, Train dice: 0.0055, Val dice: 0.0040\n",
      "9/30 - Train loss: 1.6489, Val loss: 1.6638, Train dice: 0.0064, Val dice: 0.0043\n",
      "10/30 - Train loss: 1.6406, Val loss: 1.6579, Train dice: 0.0078, Val dice: 0.0048\n",
      "11/30 - Train loss: 1.6304, Val loss: 1.6503, Train dice: 0.0101, Val dice: 0.0057\n",
      "12/30 - Train loss: 1.6179, Val loss: 1.6407, Train dice: 0.0134, Val dice: 0.0073\n",
      "13/30 - Train loss: 1.6025, Val loss: 1.6292, Train dice: 0.0183, Val dice: 0.0099\n",
      "14/30 - Train loss: 1.5838, Val loss: 1.6159, Train dice: 0.0254, Val dice: 0.0138\n",
      "15/30 - Train loss: 1.5618, Val loss: 1.6020, Train dice: 0.0356, Val dice: 0.0187\n",
      "16/30 - Train loss: 1.5377, Val loss: 1.5886, Train dice: 0.0481, Val dice: 0.0244\n",
      "17/30 - Train loss: 1.5119, Val loss: 1.5767, Train dice: 0.0627, Val dice: 0.0298\n",
      "18/30 - Train loss: 1.4848, Val loss: 1.5668, Train dice: 0.0789, Val dice: 0.0348\n",
      "19/30 - Train loss: 1.4534, Val loss: 1.5576, Train dice: 0.0992, Val dice: 0.0404\n",
      "20/30 - Train loss: 1.4136, Val loss: 1.5477, Train dice: 0.1274, Val dice: 0.0481\n",
      "21/30 - Train loss: 1.3594, Val loss: 1.5356, Train dice: 0.1693, Val dice: 0.0596\n",
      "22/30 - Train loss: 1.2852, Val loss: 1.5217, Train dice: 0.2302, Val dice: 0.0750\n",
      "23/30 - Train loss: 1.1903, Val loss: 1.5066, Train dice: 0.3105, Val dice: 0.0939\n",
      "24/30 - Train loss: 1.0756, Val loss: 1.4924, Train dice: 0.4089, Val dice: 0.1150\n",
      "25/30 - Train loss: 0.9478, Val loss: 1.4835, Train dice: 0.5181, Val dice: 0.1348\n",
      "26/30 - Train loss: 0.8144, Val loss: 1.4821, Train dice: 0.6304, Val dice: 0.1519\n",
      "27/30 - Train loss: 0.6835, Val loss: 1.4796, Train dice: 0.7367, Val dice: 0.1726\n",
      "28/30 - Train loss: 0.5625, Val loss: 1.4439, Train dice: 0.8298, Val dice: 0.2183\n",
      "29/30 - Train loss: 0.4533, Val loss: 1.4066, Train dice: 0.9077, Val dice: 0.2656\n"
     ]
    }
   ],
   "source": [
    "train_loss_frz, train_dice_coefficient_frz = [], []\n",
    "val_loss_frz, val_dice_coefficient_frz = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses, dice_coefficients = train_one_epoch(dataloaders, model, criterion, optimizer, DEVICE)\n",
    "    train_loss_frz.append(losses[\"train\"])\n",
    "    val_loss_frz.append(losses[\"val\"])\n",
    "    train_dice_coefficient_frz.append(dice_coefficients[\"train\"])\n",
    "    val_dice_coefficient_frz.append(dice_coefficients[\"val\"])\n",
    "    \n",
    "    print(f\"{epoch}/{num_epochs} - Train loss: {losses['train']:.4f}, Val loss: {losses['val']:.4f},\" + \\\n",
    "          f\" Train dice: {dice_coefficients['train']:.4f}, Val dice: {dice_coefficients['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'save_metrics_to_csv' 함수를 수정하여 파일 이름에 날짜와 시간을 포함하고, 변수명에 구분자를 추가하여 하나의 파일에 저장하도록 변경\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def save_metrics_to_csv(all_metrics, save_dir):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    # 현재 날짜와 시간을 가져옵니다.\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 결과를 저장할 데이터 프레임을 생성합니다.\n",
    "    epochs = range(len(all_metrics['train_loss_def']))\n",
    "    df = pd.DataFrame({\n",
    "        'Epoch': epochs,\n",
    "        'Train Loss (Default)': all_metrics['train_loss_def'],\n",
    "        'Validation Loss (Default)': all_metrics['val_loss_def'],\n",
    "        'Train Dice Coefficient (Default)': all_metrics['train_dice_def'],\n",
    "        'Validation Dice Coefficient (Default)': all_metrics['val_dice_def'],\n",
    "        'Train Loss (Prt)': all_metrics['train_loss_prt'],\n",
    "        'Validation Loss (Prt)': all_metrics['val_loss_prt'],\n",
    "        'Train Dice Coefficient (Prt)': all_metrics['train_dice_prt'],\n",
    "        'Validation Dice Coefficient (Prt)': all_metrics['val_dice_prt'],\n",
    "        'Train Loss (Frz)': all_metrics['train_loss_frz'],\n",
    "        'Validation Loss (Frz)': all_metrics['val_loss_frz'],\n",
    "        'Train Dice Coefficient (Frz)': all_metrics['train_dice_frz'],\n",
    "        'Validation Dice Coefficient (Frz)': all_metrics['val_dice_frz'],\n",
    "    })\n",
    "    \n",
    "    # 저장 경로 생성\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # 파일 이름 설정\n",
    "    filename = f'training_results_{current_time}.csv'\n",
    "    # 파일 저장\n",
    "    df.to_csv(os.path.join(save_dir, filename), index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = {\n",
    "    'train_loss_def': train_loss_def,\n",
    "    'val_loss_def': val_loss_def,\n",
    "    'train_dice_def': train_dice_coefficient_def,\n",
    "    'val_dice_def': val_dice_coefficient_def,\n",
    "    'train_loss_prt': train_loss_prt,\n",
    "    'val_loss_prt': val_loss_prt,\n",
    "    'train_dice_prt': train_dice_coefficient_prt,\n",
    "    'val_dice_prt': val_dice_coefficient_prt,\n",
    "    'train_loss_frz': train_loss_frz,\n",
    "    'val_loss_frz': val_loss_frz,\n",
    "    'train_dice_frz': train_dice_coefficient_frz,\n",
    "    'val_dice_frz': val_dice_coefficient_frz,\n",
    "}\n",
    "save_dir = 'summary'\n",
    "save_metrics_to_csv(all_metrics, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_proj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
