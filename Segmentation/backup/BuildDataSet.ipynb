{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 세그멘테이션 프로젝트를 위해 단계별로 구현 계획을 세웠습니다. 각 단계에 따라 코드를 구현하겠습니다.\n",
    "\n",
    "### 1. 데이터 준비 및 전처리\n",
    "- **압축 해제**: `mini_data.zip` 파일의 내용을 압축 해제하고 파일 구조를 확인합니다.\n",
    "- **데이터 분리**: 이미지와 마스크를 연결하는 CSV 파일을 사용하여 이미지와 해당 마스크를 매칭하고, 이를 훈련 데이터와 검증 데이터로 9:1 비율로 분리합니다.\n",
    "\n",
    "### 2. 모델 구성\n",
    "- **신경망 선택**: U-Net, FCN, SegNet 아키텍처를 구현하고, ResNet, VGG, EfficientNet을 각각의 백본으로 사용합니다.\n",
    "- **모델 정의**: 각 신경망에 맞게 모델을 정의하고 PyTorch에서 사용할 수 있도록 설정합니다.\n",
    "\n",
    "### 3. 훈련 및 검증\n",
    "- **손실 함수 및 최적화**: Dice Coefficient 손실 함수를 사용하고, 옵티마이저로는 Adam을 선택합니다.\n",
    "- **지표 계산**: 각 에포크마다 Dice Coefficient와 IOU를 계산합니다.\n",
    "- **로그 저장**: 훈련 중 각 에포크의 결과를 \"신경망이름_현재시간.csv\"로 저장합니다.\n",
    "- **모델 저장**: 각 신경망의 가장 좋은 모델을 \"신경망이름_현재시간.pth\" 파일로 저장합니다.\n",
    "\n",
    "### 4. 결과 분석 및 시각화\n",
    "- **손실 및 지표 그래프**: LOSS, Dice, IOU 값을 시각화하여 그래프로 나타냅니다.\n",
    "- **인터랙티브 시각화**: Jupyter Notebook에서 입력 이미지, 입력 마스크, 예측 마스크를 비교할 수 있도록 시각화 합니다.\n",
    "\n",
    "### 5. 실행 및 검증\n",
    "- **모델 로드**: 훈련된 모델을 로드하고, 검증 데이터를 사용하여 모델 성능을 평가합니다.\n",
    "- **결과 검증**: 시각적으로 결과를 검증하고 모델의 성능을 평가합니다.\n",
    "\n",
    "이 계획에 따라 코드를 구현하겠습니다. 우선, `mini_data.zip` 파일을 압축 해제하고 데이터 구조를 확인한 다음, 데이터를 분리하는 코드부터 작성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>MaskId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230930_21h03m03s_B01_1.png</td>\n",
       "      <td>20230930_21h03m03s_B01_1_mask.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230930_21h03m03s_B01_2.png</td>\n",
       "      <td>20230930_21h03m03s_B01_2_mask.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230930_21h03m06s_B02_1.png</td>\n",
       "      <td>20230930_21h03m06s_B02_1_mask.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230930_21h03m06s_B02_2.png</td>\n",
       "      <td>20230930_21h03m06s_B02_2_mask.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230930_21h03m06s_B02_3.png</td>\n",
       "      <td>20230930_21h03m06s_B02_3_mask.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ImageId                             MaskId\n",
       "0  20230930_21h03m03s_B01_1.png  20230930_21h03m03s_B01_1_mask.png\n",
       "1  20230930_21h03m03s_B01_2.png  20230930_21h03m03s_B01_2_mask.png\n",
       "2  20230930_21h03m06s_B02_1.png  20230930_21h03m06s_B02_1_mask.png\n",
       "3  20230930_21h03m06s_B02_2.png  20230930_21h03m06s_B02_2_mask.png\n",
       "4  20230930_21h03m06s_B02_3.png  20230930_21h03m06s_B02_3_mask.png"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '../DataSet/BeadSegmentation/'\n",
    "# train.csv 파일 경로\n",
    "csv_path = os.path.join(data_path, 'mini_data', 'train.csv')\n",
    "\n",
    "# CSV 파일 로드\n",
    "data_df = pd.read_csv(csv_path)\n",
    "data_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 분리 단계\n",
    "- **데이터셋 셔플링**: 데이터를 무작위로 섞어서 모델이 특정 순서에 의존하지 않도록 합니다.\n",
    "- **분리**: 전체 데이터셋을 90%는 훈련용으로, 나머지 10%는 검증용으로 분리합니다.\n",
    "- **파일 저장**: 분리된 데이터를 각각의 폴더(훈련 및 검증)에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                         ImageId                             MaskId\n",
       " 37  20230930_21h03m42s_B11_2.png  20230930_21h03m42s_B11_2_mask.png\n",
       " 12  20230930_21h03m13s_B04_3.png  20230930_21h03m13s_B04_3_mask.png\n",
       " 19  20230930_21h03m20s_B06_2.png  20230930_21h03m20s_B06_2_mask.png\n",
       " 4   20230930_21h03m06s_B02_3.png  20230930_21h03m06s_B02_3_mask.png\n",
       " 25  20230930_21h03m31s_B08_2.png  20230930_21h03m31s_B08_2_mask.png,\n",
       "                          ImageId                             MaskId\n",
       " 27  20230930_21h03m31s_B08_4.png  20230930_21h03m31s_B08_4_mask.png\n",
       " 40  20230930_21h03m46s_B12_1.png  20230930_21h03m46s_B12_1_mask.png\n",
       " 26  20230930_21h03m31s_B08_3.png  20230930_21h03m31s_B08_3_mask.png\n",
       " 43  20230930_21h03m46s_B12_4.png  20230930_21h03m46s_B12_4_mask.png\n",
       " 24  20230930_21h03m31s_B08_1.png  20230930_21h03m31s_B08_1_mask.png,\n",
       " 43,\n",
       " 5)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터셋을 훈련용과 검증용으로 분리 (90% 훈련, 10% 검증)\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# 훈련용과 검증용 데이터 프레임 확인\n",
    "train_df.head(), val_df.head(), len(train_df), len(val_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 폴더 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['20230930_21h03m13s_B04_2.png',\n",
       "  '20230930_21h03m13s_B04_4.png',\n",
       "  '20230930_21h03m20s_B06_3.png',\n",
       "  '20230930_21h03m50s_B13_2.png',\n",
       "  '20230930_21h03m35s_B09_1.png',\n",
       "  '20230930_21h03m35s_B09_3.png',\n",
       "  '20230930_21h03m06s_B02_3.png',\n",
       "  '20230930_21h03m39s_B10_3.png',\n",
       "  '20230930_21h03m09s_B03_3.png',\n",
       "  '20230930_21h03m50s_B13_3.png',\n",
       "  '20230930_21h03m46s_B12_3.png',\n",
       "  '20230930_21h03m06s_B02_4.png',\n",
       "  '20230930_21h03m42s_B11_1.png',\n",
       "  '20230930_21h03m20s_B06_4.png',\n",
       "  '20230930_21h03m39s_B10_2.png',\n",
       "  '20230930_21h03m06s_B02_1.png',\n",
       "  '20230930_21h03m20s_B06_1.png',\n",
       "  '20230930_21h03m50s_B13_1.png',\n",
       "  '20230930_21h03m42s_B11_3.png',\n",
       "  '20230930_21h03m09s_B03_2.png',\n",
       "  '20230930_21h03m06s_B02_2.png',\n",
       "  '20230930_21h03m42s_B11_4.png',\n",
       "  '20230930_21h03m35s_B09_2.png',\n",
       "  '20230930_21h03m03s_B01_2.png',\n",
       "  '20230930_21h03m16s_B05_4.png',\n",
       "  '20230930_21h03m13s_B04_1.png',\n",
       "  '20230930_21h03m35s_B09_4.png',\n",
       "  '20230930_21h03m42s_B11_2.png',\n",
       "  '20230930_21h03m16s_B05_3.png',\n",
       "  '20230930_21h03m09s_B03_1.png',\n",
       "  '20230930_21h03m20s_B06_2.png',\n",
       "  '20230930_21h03m03s_B01_1.png',\n",
       "  '20230930_21h03m39s_B10_4.png',\n",
       "  '20230930_21h03m24s_B07_2.png',\n",
       "  '20230930_21h03m09s_B03_4.png',\n",
       "  '20230930_21h03m31s_B08_2.png',\n",
       "  '20230930_21h03m24s_B07_1.png',\n",
       "  '20230930_21h03m13s_B04_3.png',\n",
       "  '20230930_21h03m16s_B05_2.png',\n",
       "  '20230930_21h03m16s_B05_1.png',\n",
       "  '20230930_21h03m39s_B10_1.png',\n",
       "  '20230930_21h03m46s_B12_2.png',\n",
       "  '20230930_21h03m50s_B13_4.png'],\n",
       " ['20230930_21h03m16s_B05_1_mask.png',\n",
       "  '20230930_21h03m35s_B09_2_mask.png',\n",
       "  '20230930_21h03m24s_B07_2_mask.png',\n",
       "  '20230930_21h03m20s_B06_1_mask.png',\n",
       "  '20230930_21h03m13s_B04_1_mask.png',\n",
       "  '20230930_21h03m09s_B03_4_mask.png',\n",
       "  '20230930_21h03m20s_B06_2_mask.png',\n",
       "  '20230930_21h03m50s_B13_3_mask.png',\n",
       "  '20230930_21h03m35s_B09_3_mask.png',\n",
       "  '20230930_21h03m42s_B11_4_mask.png',\n",
       "  '20230930_21h03m16s_B05_3_mask.png',\n",
       "  '20230930_21h03m39s_B10_3_mask.png',\n",
       "  '20230930_21h03m03s_B01_1_mask.png',\n",
       "  '20230930_21h03m24s_B07_1_mask.png',\n",
       "  '20230930_21h03m50s_B13_4_mask.png',\n",
       "  '20230930_21h03m39s_B10_1_mask.png',\n",
       "  '20230930_21h03m42s_B11_2_mask.png',\n",
       "  '20230930_21h03m31s_B08_2_mask.png',\n",
       "  '20230930_21h03m09s_B03_1_mask.png',\n",
       "  '20230930_21h03m39s_B10_4_mask.png',\n",
       "  '20230930_21h03m46s_B12_3_mask.png',\n",
       "  '20230930_21h03m35s_B09_1_mask.png',\n",
       "  '20230930_21h03m16s_B05_2_mask.png',\n",
       "  '20230930_21h03m50s_B13_2_mask.png',\n",
       "  '20230930_21h03m13s_B04_2_mask.png',\n",
       "  '20230930_21h03m06s_B02_4_mask.png',\n",
       "  '20230930_21h03m39s_B10_2_mask.png',\n",
       "  '20230930_21h03m42s_B11_3_mask.png',\n",
       "  '20230930_21h03m13s_B04_4_mask.png',\n",
       "  '20230930_21h03m06s_B02_2_mask.png',\n",
       "  '20230930_21h03m35s_B09_4_mask.png',\n",
       "  '20230930_21h03m20s_B06_3_mask.png',\n",
       "  '20230930_21h03m46s_B12_2_mask.png',\n",
       "  '20230930_21h03m09s_B03_2_mask.png',\n",
       "  '20230930_21h03m16s_B05_4_mask.png',\n",
       "  '20230930_21h03m50s_B13_1_mask.png',\n",
       "  '20230930_21h03m03s_B01_2_mask.png',\n",
       "  '20230930_21h03m42s_B11_1_mask.png',\n",
       "  '20230930_21h03m06s_B02_1_mask.png',\n",
       "  '20230930_21h03m06s_B02_3_mask.png',\n",
       "  '20230930_21h03m09s_B03_3_mask.png',\n",
       "  '20230930_21h03m13s_B04_3_mask.png',\n",
       "  '20230930_21h03m20s_B06_4_mask.png'],\n",
       " ['20230930_21h03m31s_B08_1.png',\n",
       "  '20230930_21h03m46s_B12_4.png',\n",
       "  '20230930_21h03m31s_B08_3.png',\n",
       "  '20230930_21h03m31s_B08_4.png',\n",
       "  '20230930_21h03m46s_B12_1.png'],\n",
       " ['20230930_21h03m31s_B08_3_mask.png',\n",
       "  '20230930_21h03m31s_B08_1_mask.png',\n",
       "  '20230930_21h03m46s_B12_1_mask.png',\n",
       "  '20230930_21h03m31s_B08_4_mask.png',\n",
       "  '20230930_21h03m46s_B12_4_mask.png'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 데이터 세트 유형 선택 ('mini_data' 또는 'original_data')\n",
    "data_type = 'mini_data'  # 또는 'original_data'\n",
    "\n",
    "# 데이터셋 경로 설정\n",
    "dataset_path = os.path.join(data_path, data_type)\n",
    "\n",
    "# 훈련 및 검증 데이터 폴더 생성\n",
    "train_dir = os.path.join(data_path, 'train')\n",
    "val_dir = os.path.join(data_path, 'val')\n",
    "\n",
    "# 훈련 및 검증 폴더 내의 이미지와 마스크 폴더 생성\n",
    "train_img_dir = os.path.join(train_dir, 'images')\n",
    "train_mask_dir = os.path.join(train_dir, 'masks')\n",
    "val_img_dir = os.path.join(val_dir, 'images')\n",
    "val_mask_dir = os.path.join(val_dir, 'masks')\n",
    "\n",
    "# 기존 폴더가 있을 경우 삭제\n",
    "for dir_path in [train_dir, val_dir]:\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "# 이미지와 마스크 폴더 생성\n",
    "for sub_dir in [train_img_dir, train_mask_dir, val_img_dir, val_mask_dir]:\n",
    "    os.makedirs(sub_dir)\n",
    "\n",
    "# 훈련용 데이터 복사\n",
    "for idx, row in train_df.iterrows():\n",
    "    # 원본 이미지와 마스크 경로\n",
    "    src_image_path = os.path.join(dataset_path, 'images', row['ImageId'])\n",
    "    src_mask_path = os.path.join(dataset_path, 'masks', row['MaskId'])\n",
    "    # 대상 이미지와 마스크 경로\n",
    "    dst_image_path = os.path.join(train_img_dir, row['ImageId'])\n",
    "    dst_mask_path = os.path.join(train_mask_dir, row['MaskId'])\n",
    "    \n",
    "    # 파일 복사\n",
    "    shutil.copy(src_image_path, dst_image_path)\n",
    "    shutil.copy(src_mask_path, dst_mask_path)\n",
    "\n",
    "# 검증용 데이터 복사\n",
    "for idx, row in val_df.iterrows():\n",
    "    # 원본 이미지와 마스크 경로\n",
    "    src_image_path = os.path.join(dataset_path, 'images', row['ImageId'])\n",
    "    src_mask_path = os.path.join(dataset_path, 'masks', row['MaskId'])\n",
    "    # 대상 이미지와 마스크 경로\n",
    "    dst_image_path = os.path.join(val_img_dir, row['ImageId'])\n",
    "    dst_mask_path = os.path.join(val_mask_dir, row['MaskId'])\n",
    "    \n",
    "    # 파일 복사\n",
    "    shutil.copy(src_image_path, dst_image_path)\n",
    "    shutil.copy(src_mask_path, dst_mask_path)\n",
    "\n",
    "# 확인\n",
    "os.listdir(train_img_dir), os.listdir(train_mask_dir), os.listdir(val_img_dir), os.listdir(val_mask_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실 함수 정의\n",
    "손실 함수는 실제 마스크와 예측 마스크 간의 유사도를 Dice로 계산하고, 이진 분류 손실을 BCE로 계산하여 더합니다. 이는 모델이 배경과 전경을 보다 정확히 분리할 수 있도록 도와줍니다.\n",
    "- **Dice Loss**: 주로 사용되는 세그멘테이션 손실 함수 중 하나로, 예측된 마스크와 실제 마스크 사이의 유사도를 측정합니다.\n",
    "- **Binary Cross Entropy Loss**: 픽셀 수준에서 실제 값과 예측 값 사이의 차이를 계산합니다.\n",
    "- **결합 손실 함수**: 이 두 손실을 결합하여 최종 손실을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        # 입력과 타겟은 [N, 1, H, W] 형태로 가정합니다.\n",
    "        # inputs -> 모델의 sigmoid 출력 결과\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        \n",
    "        # Flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Dice Loss\n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth) / (inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        # Binary Cross Entropy Loss\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        \n",
    "        # 결합된 손실\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 정의 및 훈련\n",
    "- **모델 아키텍처 정의**: U-Net, FCN, SegNet에 대한 아키텍처를 정의하고, ResNet, VGG, EfficientNet을 백본으로 선택할 수 있게 설정합니다.\n",
    "- **손실 함수 및 최적화**: Dice Coefficient를 손실 함수로 사용하고, SGD와 Adam 중에서 옵티마이저를 선택할 수 있게 설정합니다.\n",
    "- **훈련 루프**: 지정된 에포크 수(30 에포크) 동안 모델을 훈련하고 각 에포크의 결과를 기록합니다.\n",
    "- **성능 지표 기록**: 훈련 동안 Dice Coefficient와 IOU 지표를 계산하여 기록합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습 준비\n",
    "1. **이미지와 마스크 로딩**: PyTorch에서는 `Dataset` 클래스를 사용하여 이미지와 해당 마스크를 로드하고, 필요한 변환(예: 정규화, 크기 조정)을 적용합니다.\n",
    "2. **이진 분류 세팅**: 마스크는 이진 형식이므로, 모델의 출력 채널은 1로 설정합니다(클래스가 하나뿐이므로). \n",
    "3. **손실 함수 설정**: Dice Coefficient는 이진 세그멘테이션에 적합한 손실 함수입니다. 추가적으로, 배경과 세그멘테이션 대상의 불균형을 조정하기 위해 Binary Cross Entropy (BCE)와 결합할 수 있습니다.\n",
    "\n",
    "### 데이터 로딩 및 변환 구현\n",
    "- PyTorch의 `torchvision.transforms`를 사용하여 입력 이미지와 마스크에 대한 전처리를 설정합니다.\n",
    "- 이미지는 RGB 채널을 사용하고, 마스크는 그레이스케일로 처리합니다.\n",
    "- 데이터 로더(`DataLoader`)를 사용하여 배치 처리를 구성합니다.\n",
    "\n",
    "### 모델 아키텍처 설정\n",
    "- 백본 네트워크로 ResNet, VGG, EfficientNet 중 선택 가능하게 구성합니다.\n",
    "- 주요 세그멘테이션 아키텍처로 U-Net, FCN, SegNet을 구현합니다. 이들 각각에 대해 백본 네트워크를 적용하는 방법을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, mask_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (DataFrame): 이미지와 마스크의 파일 이름을 포함하는 데이터프레임.\n",
    "            image_dir (string): 이미지 파일의 디렉토리 경로.\n",
    "            mask_dir (string): 마스크 파일의 디렉토리 경로.\n",
    "            transform (callable, optional): 샘플에 적용할 선택적 변형.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지 파일과 마스크 파일 경로\n",
    "        img_name = os.path.join(self.image_dir, self.df.iloc[idx, 0])\n",
    "        mask_name = os.path.join(self.mask_dir, self.df.iloc[idx, 1])\n",
    "        \n",
    "        # 이미지와 마스크 로딩\n",
    "        image = cv2.imread(img_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_name, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # 히스토그램 평활화 적용\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 마스크는 픽셀 값이 0과 255로 이진화\n",
    "        mask = (mask > 128).astype(np.float32)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# 데이터셋 트랜스폼 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 데이터셋 및 데이터 로더 생성\n",
    "train_dataset = SegmentationDataset(train_df, train_img_dir, train_mask_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# 검증 데이터셋과 데이터 로더 설정\n",
    "val_dataset = SegmentationDataset(val_df, val_img_dir, val_mask_dir, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Resnet을 백본으로 사용하는 U-Net 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, backbone='resnet', pretrained=True, num_classes=1):\n",
    "        super(ResUNet, self).__init__()\n",
    "        if backbone == 'resnet':\n",
    "            self.base_model = models.resnet34(pretrained=pretrained)\n",
    "            self.base_layers = list(self.base_model.children())\n",
    "            self.layer1 = nn.Sequential(*self.base_layers[:3])  # size=(N, 64, H/2, W/2)\n",
    "            self.layer2 = nn.Sequential(*self.base_layers[3:5])  # size=(N, 128, H/4, W/4)\n",
    "            self.layer3 = self.base_layers[5]  # size=(N, 256, H/8, W/8)\n",
    "            self.layer4 = self.base_layers[6]  # size=(N, 512, H/16, W/16)\n",
    "\n",
    "            # 확장 경로\n",
    "            self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "            self.conv_up1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "            self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "            self.conv_up2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "            self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "            self.conv_up3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "            self.up4 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
    "            self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 계약 경로\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "\n",
    "        # 확장 경로 + skip connection\n",
    "        x = self.up1(x4)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/club8080/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/club8080/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로더 및 모델 인스턴스화\n",
    "\n",
    "model = ResUNet(backbone='resnet', pretrained=True, num_classes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.VGG 백본을 사용하는 U-Net 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGUNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, pretrained=True):\n",
    "        super(VGGUNet, self).__init__()\n",
    "        self.base_model = models.vgg16(pretrained=pretrained).features\n",
    "        self.encoder1 = self.base_model[:5]   # 64\n",
    "        self.encoder2 = self.base_model[5:10] # 128\n",
    "        self.encoder3 = self.base_model[10:17] # 256\n",
    "        self.encoder4 = self.base_model[17:24] # 512\n",
    "        self.encoder5 = self.base_model[24:] # 512\n",
    "\n",
    "        self.upconv5 = nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)\n",
    "        self.conv_decode5 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv_decode4 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv_decode3 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv_decode2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(enc1)\n",
    "        enc3 = self.encoder3(enc2)\n",
    "        enc4 = self.encoder4(enc3)\n",
    "        enc5 = self.encoder5(enc4)\n",
    "\n",
    "        up5 = self.upconv5(enc5)\n",
    "        dec5 = torch.cat([up5, enc4], dim=1)\n",
    "        dec5 = self.conv_decode5(dec5)\n",
    "        \n",
    "        up4 = self.upconv4(dec5)\n",
    "        dec4 = torch.cat([up4, enc3], dim=1)\n",
    "        dec4 = self.conv_decode4(dec4)\n",
    "        \n",
    "        up3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat([up3, enc2], dim=1)\n",
    "        dec3 = self.conv_decode3(dec3)\n",
    "        \n",
    "        up2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat([up2, enc1], dim=1)\n",
    "        dec2 = self.conv_decode2(dec2)\n",
    "        \n",
    "        up1 = self.upconv1(dec2)\n",
    "        return self.final_conv(up1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.EfficientNet 백본을 사용하는 U-Net 모델 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "class EfficientNetUNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, pretrained=True):\n",
    "        super(EfficientNetUNet, self).__init__()\n",
    "        self.base_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        self.encoder_blocks = list(self.base_model.children())[0]\n",
    "\n",
    "        self.upconv5 = nn.ConvTranspose2d(1280, 112, kernel_size=2, stride=2)\n",
    "        self.conv_decode5 = nn.Conv2d(112 + 320, 112, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv4 = nn.ConvTranspose2d(112, 40, kernel_size=2, stride=2)\n",
    "        self.conv_decode4 = nn.Conv2d(40 + 48, 40, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(40, 24, kernel_size=2, stride=2)\n",
    "        self.conv_decode3 = nn.Conv2d(24 + 32, 24, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(24, 16, kernel_size=2, stride=2)\n",
    "        self.conv_decode2 = nn.Conv2d(16 + 24, 16, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2)\n",
    "        self.final_conv = nn.Conv2d(8, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # EfficientNet의 각 블록 출력\n",
    "        x_skip = []\n",
    "        x = self.base_model._conv_stem(x)\n",
    "        x = self.base_model._bn0(x)\n",
    "        x = self.base_model._swish(x)\n",
    "        \n",
    "        for idx, block in enumerate(self.base_model._blocks):\n",
    "            x = block(x)\n",
    "            if idx in {2, 4, 10, 15}:\n",
    "                x_skip.append(x)\n",
    "        \n",
    "        x = self.base_model._conv_head(x)\n",
    "        x = self.base_model._bn1(x)\n",
    "        x = self.base_model._swish(x)\n",
    "        \n",
    "        # 업샘플링과 skip connection\n",
    "        up5 = self.upconv5(x)\n",
    "        dec5 = torch.cat([up5, x_skip[3]], dim=1)\n",
    "        dec5 = self.conv_decode5(dec5)\n",
    "        \n",
    "        up4 = self.upconv4(dec5)\n",
    "        dec4 = torch.cat([up4, x_skip[2]], dim=1)\n",
    "        dec4 = self.conv_decode4(dec4)\n",
    "        \n",
    "        up3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat([up3, x_skip[1]], dim=1)\n",
    "        dec3 = self.conv_decode3(dec3)\n",
    "        \n",
    "        up2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat([up2, x_skip[0]], dim=1)\n",
    "        dec2 = self.conv_decode2(dec2)\n",
    "        \n",
    "        up1 = self.upconv1(dec2)\n",
    "        return self.final_conv(up1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ResNet 백본을 사용하는 FCN 모델 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models.segmentation as segmentation\n",
    "\n",
    "class FCNResNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, pretrained_backbone=True):\n",
    "        super(FCNResNet, self).__init__()\n",
    "        # FCN 구성에 사용될 ResNet 기반 FCN 모델 로드\n",
    "        self.fcn = segmentation.fcn_resnet50(pretrained=pretrained_backbone)\n",
    "        # 마지막 분류 컨볼루션 레이어를 우리의 클래스 수에 맞게 조정\n",
    "        self.fcn.classifier[4] = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fcn(x)['out']\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/club8080/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=FCN_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스화 및 테스트\n",
    "model = FCNResNet(num_classes=1, pretrained_backbone=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. VGG 백본을 사용하는 FCN 모델 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNVGG(nn.Module):\n",
    "    def __init__(self, num_classes=1, pretrained=True):\n",
    "        super(FCNVGG, self).__init__()\n",
    "        vgg = models.vgg16(pretrained=pretrained).features\n",
    "        \n",
    "        self.features = nn.Sequential(*list(vgg.children())[:-1])  # 마지막 maxpool 제외\n",
    "        self.fcn = nn.Sequential(\n",
    "            nn.Conv2d(512, 4096, kernel_size=7),  # FC 레이어를 컨볼루션으로 대체\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(4096, 4096, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(4096, num_classes, kernel_size=1)\n",
    "        )\n",
    "        self.up = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, stride=32, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.fcn(x)\n",
    "        x = self.up(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. EfficientNet 백본을 사용하는 FCN 모델 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNEfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, pretrained=True):\n",
    "        super(FCNEfficientNet, self).__init__()\n",
    "        self.base_model = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "        \n",
    "        # EfficientNet의 각 블록에서 특징 맵을 추출\n",
    "        self.encoder = nn.ModuleList(list(self.base_model.children())[:-2])\n",
    "        \n",
    "        self.fcn = nn.Sequential(\n",
    "            nn.Conv2d(1792, num_classes, kernel_size=1)  # 특징 맵을 클래스 수에 맞게 조정\n",
    "        )\n",
    "        self.up = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, stride=32, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "        x = self.fcn(x)\n",
    "        x = self.up(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. ResNet 백본을 사용하는 SegNet 모델 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetSegNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, pretrained=True):\n",
    "        super(ResNetSegNet, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. VGG 백본을 사용하는 SegNet 모델 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGSegNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, pretrained=True):\n",
    "        super(VGGSegNet, self).__init__()\n",
    "        vgg = models.vgg16(pretrained=pretrained).features\n",
    "        \n",
    "        self.encoder = nn.Sequential(*list(vgg.children())[:-1])  # 마지막 MaxPool 제외\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. EfficientNet 백본을 사용하는 SegNet 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetSegNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, pretrained=True):\n",
    "        super(EfficientNetSegNet, self).__init__()\n",
    "        efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        self.encoder = nn.Sequential(*list(efficientnet.children())[:-2])\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(320, 112, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(112, 40, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(40, 24, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(24, 16, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 파이프라인 설정\n",
    "\n",
    "- **데이터 전처리 및 로더 설정**: 히스토그램 평활화, 크기 조정, 텐서 변환을 포함합니다.\n",
    "- **모델 정의**: 9개의 모델 각각을 정의하고 GPU에 할당합니다.\n",
    "- **손실 함수와 옵티마이저 설정**: Dice Coefficient 기반 BCE와 선택적 크로스엔트로피, 옵티마이저로 SGD와 Adam 선택 가능하도록 설정합니다.\n",
    "- **훈련 루프 실행**: 에포크마다의 손실, Dice Coefficient, IOU를 계산하고 로깅합니다.\n",
    "- **결과 저장 및 시각화**: 각 모델의 성능을 csv 파일과 그래프로 저장하고 시각화합니다.\n",
    "- **모델 저장**: 각 모델의 최고 성능을 가진 상태를 파일로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dice_coeff(outputs, labels, smooth=1):\n",
    "    # outputs을 확률값으로 변환하기 위해 sigmoid 함수 적용\n",
    "    outputs = torch.sigmoid(outputs)\n",
    "\n",
    "    # 출력과 라벨 텐서를 평탄화(flatten)\n",
    "    outputs_flat = outputs.view(-1)\n",
    "    labels_flat = labels.view(-1)\n",
    "\n",
    "    intersection = (outputs_flat * labels_flat).sum()\n",
    "    dice = (2. * intersection + smooth) / (outputs_flat.sum() + labels_flat.sum() + smooth)\n",
    "\n",
    "    return dice.item()\n",
    "\n",
    "def compute_iou(outputs, labels, smooth=1):\n",
    "    # outputs을 확률값으로 변환하기 위해 sigmoid 함수 적용\n",
    "    outputs = torch.sigmoid(outputs)\n",
    "\n",
    "    # 출력과 라벨 텐서를 평탄화(flatten)\n",
    "    outputs_flat = outputs.view(-1)\n",
    "    labels_flat = labels.view(-1)\n",
    "\n",
    "    intersection = (outputs_flat * labels_flat).sum()\n",
    "    union = outputs_flat.sum() + labels_flat.sum() - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "\n",
    "    return iou.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 훈련 및 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_name):\n",
    "    since = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    summary_path = f\"./summary/{model_name}_{since}.csv\"\n",
    "    checkpoint_path = f\"/home/club8080/Downloads/trained_model/{model_name}_{since}.pth\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 각 에포크별 훈련 및 검증 단계\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_dice = 0.0\n",
    "            running_iou = 0.0\n",
    "            total_batches = len(train_loader if phase == 'train' else val_loader)\n",
    "\n",
    "            # 데이터 반복\n",
    "            for inputs, labels in (train_loader if phase == 'train' else val_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # 매개변수 그라디언트를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 순전파\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    dice = compute_dice_coeff(outputs, labels)\n",
    "                    iou = compute_iou(outputs, labels)\n",
    "\n",
    "                    # 역전파 + 최적화\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # 통계\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_dice += dice * inputs.size(0)\n",
    "                running_iou += iou * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / total_batches\n",
    "            epoch_dice = running_dice / total_batches\n",
    "            epoch_iou = running_iou / total_batches\n",
    "            results.append([epoch + 1, phase, epoch_loss, epoch_dice, epoch_iou])\n",
    "\n",
    "            print('{} Loss: {:.4f} Dice: {:.4f} IOU: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_dice, epoch_iou))\n",
    "\n",
    "            # 모델 저장\n",
    "            if phase == 'val' and epoch_iou > best_iou:\n",
    "                best_iou = epoch_iou\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "    \n",
    "    # 결과를 DataFrame으로 저장 후 CSV로 저장\n",
    "    results_df = pd.DataFrame(results, columns=['Epoch', 'Phase', 'Loss', 'Dice', 'IOU'])\n",
    "    results_df.to_csv(summary_path, index=False)\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/club8080/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/club8080/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@3712.725] global loadsave.cpp:248 findDecoder imread_('../DataSet/BeadSegmentation/train/20230930_21h03m06s_B02_4.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@3712.726] global loadsave.cpp:248 findDecoder imread_('../DataSet/BeadSegmentation/train/20230930_21h03m13s_B04_1.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@3712.726] global loadsave.cpp:248 findDecoder imread_('../DataSet/BeadSegmentation/train/20230930_21h03m09s_B03_2.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/club8080/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/club8080/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/club8080/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_6507/1346661837.py\", line 30, in __getitem__\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ncv2.error: OpenCV(4.9.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     14\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResnet_Unet\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 모델 이름 설정\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m total_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m val_loader)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 데이터 반복\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31merror\u001b[0m: Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/club8080/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/club8080/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/club8080/.pyenv/versions/3.11.5/envs/my_proj_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_6507/1346661837.py\", line 30, in __getitem__\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ncv2.error: OpenCV(4.9.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n\n"
     ]
    }
   ],
   "source": [
    "# 모델, 데이터 로더, 손실 함수, 옵티마이저 준비\n",
    "model = ResUNet(backbone='resnet', pretrained=True, num_classes=1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 손실 함수 설정\n",
    "criterion = DiceBCELoss()  # 가정: DiceBCELoss가 정의되어 있음\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 훈련 실행\n",
    "num_epochs = 30\n",
    "model_name = \"Resnet_Unet\"  # 모델 이름 설정\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 성능 비교 및 그래프 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(model_names):\n",
    "    fig, ax = plt.subplots(len(model_names), 3, figsize=(15, 5 * len(model_names)))\n",
    "\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        df = pd.read_csv(f\"./summary/{model_name}.csv\")\n",
    "        ax[i, 0].plot(df[df['Phase'] == 'train']['Epoch'], df[df['Phase'] == 'train']['Loss'], label='Train Loss')\n",
    "        ax[i, 0].plot(df[df['Phase'] == 'val']['Epoch'], df[df['Phase'] == 'val']['Loss'], label='Val Loss')\n",
    "        ax[i, 1].plot(df[df['Phase'] == 'train']['Epoch'], df[df['Phase'] == 'train']['Dice'], label='Train Dice')\n",
    "        ax[i, 1].plot(df[df['Phase'] == 'val']['Epoch'], df[df['Phase'] == 'val']['Dice'], label='Val Dice')\n",
    "        ax[i, 2].plot(df[df['Phase'] == 'train']['Epoch'], df[df['Phase'] == 'train']['IOU'], label='Train IOU')\n",
    "        ax[i, 2].plot(df[df['Phase'] == 'val']['Epoch'], df[df['Phase'] == 'val']['IOU'], label='Val IOU')\n",
    "        \n",
    "        for j in range(3):\n",
    "            ax[i, j].set_xlabel('Epoch')\n",
    "            ax[i, j].set_ylabel('Value')\n",
    "            ax[i, j].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_proj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
